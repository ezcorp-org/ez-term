# Architecture

Complete architecture documentation for ez - the AI-powered CLI command generator.

## Table of Contents

- [Overview](#overview)
- [System Architecture](#system-architecture)
- [Module Structure](#module-structure)
- [Data Flow](#data-flow)
- [Key Components](#key-components)
- [Design Decisions](#design-decisions)
- [Extension Points](#extension-points)

## Overview

Ez is designed as a modular, extensible CLI tool that uses LLMs to generate context-aware shell commands.

### Core Principles

1. **Modularity** - Each component has a single responsibility
2. **Extensibility** - Easy to add new backends, detectors, or features
3. **Testability** - Comprehensive test coverage with clear boundaries
4. **Privacy** - All context gathering is local-only
5. **Safety** - Commands are generated with safety-first principles

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Input                           â”‚
â”‚                    (CLI Arguments/Query)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CLI Parser (clap)                       â”‚
â”‚                   Parse flags & arguments                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Configuration Layer                       â”‚
â”‚          â€¢ Load ~/.config/ez-term/config.toml               â”‚
â”‚          â€¢ Merge with environment variables                  â”‚
â”‚          â€¢ Determine backend & model                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Context Detection Layer                    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ System Info    â”‚  â”‚ Tool         â”‚  â”‚ Context        â”‚ â”‚
â”‚  â”‚                â”‚  â”‚ Detection    â”‚  â”‚ Agent          â”‚ â”‚
â”‚  â”‚ â€¢ OS           â”‚  â”‚              â”‚  â”‚                â”‚ â”‚
â”‚  â”‚ â€¢ Architecture â”‚  â”‚ â€¢ PATH scan  â”‚  â”‚ â€¢ Git context  â”‚ â”‚
â”‚  â”‚ â€¢ Shell        â”‚  â”‚ â€¢ History    â”‚  â”‚ â€¢ Docker       â”‚ â”‚
â”‚  â”‚ â€¢ Memory       â”‚  â”‚ â€¢ Freq cmds  â”‚  â”‚ â€¢ NPM          â”‚ â”‚
â”‚  â”‚ â€¢ OS guidance  â”‚  â”‚              â”‚  â”‚ â€¢ Python       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â€¢ Shell RC     â”‚ â”‚
â”‚                                         â”‚ â€¢ Cargo        â”‚ â”‚
â”‚                                         â”‚ â€¢ K8s          â”‚ â”‚
â”‚                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Prompt Construction                      â”‚
â”‚  â€¢ Base system prompt (safety rules, format)                â”‚
â”‚  â€¢ System context (OS, tools, installed software)           â”‚
â”‚  â€¢ Agentic context (query-specific: git, docker, etc.)      â”‚
â”‚  â€¢ User query                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM Client Layer                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Ollama   â”‚    â”‚  Groq    â”‚    â”‚ OpenAI   â”‚             â”‚
â”‚  â”‚ (Local)  â”‚    â”‚ (Cloud)  â”‚    â”‚ (Cloud)  â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                              â”‚
â”‚  â€¢ Generate and collect response                             â”‚
â”‚  â€¢ Streaming support (future)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Response Processing                       â”‚
â”‚  â€¢ Parse JSON response                                       â”‚
â”‚  â€¢ Extract command & description                             â”‚
â”‚  â€¢ Fallback to plain text if JSON fails                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Output Layer                            â”‚
â”‚  â€¢ Display description (ðŸ’¡)                                  â”‚
â”‚  â€¢ Output command (for shell wrapper)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shell Integration                         â”‚
â”‚  â€¢ zsh: print -z (inject into line buffer)                  â”‚
â”‚  â€¢ bash: history -s + manual up-arrow                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Module Structure

### Core Modules

```
src/
â”œâ”€â”€ main.rs              # Entry point, orchestration
â”œâ”€â”€ cli.rs               # CLI argument parsing
â”œâ”€â”€ config.rs            # Configuration management
â”œâ”€â”€ system_info.rs       # OS & system detection
â”œâ”€â”€ tool_detection.rs    # Installed tool scanning
â”œâ”€â”€ context_agent.rs     # Agentic context fetching
â”œâ”€â”€ llm_client.rs        # Multi-backend LLM client
â””â”€â”€ update.rs            # Self-update functionality
```

### Module Dependencies

```
main.rs
  â”œâ”€> cli.rs (argument parsing)
  â”œâ”€> config.rs (load configuration)
  â”œâ”€> system_info.rs (detect OS)
  â”œâ”€> tool_detection.rs (scan tools)
  â”œâ”€> context_agent.rs (fetch agentic context)
  â”œâ”€> llm_client.rs (call LLM)
  â””â”€> update.rs (self-update)

llm_client.rs
  â””â”€> (external) reqwest, serde_json

context_agent.rs
  â”œâ”€> (external) std::process::Command
  â””â”€> (external) std::fs
```

## Data Flow

### 1. Command Generation Flow

```
User Query
    â”‚
    â”œâ”€> Parse CLI args (cli.rs)
    â”‚
    â”œâ”€> Load config (config.rs)
    â”‚       â”œâ”€> Read ~/.config/ez-term/config.toml
    â”‚       â””â”€> Merge with env vars
    â”‚
    â”œâ”€> Detect System (system_info.rs)
    â”‚       â”œâ”€> OS, Architecture
    â”‚       â”œâ”€> Shell type
    â”‚       â””â”€> Memory info
    â”‚
    â”œâ”€> Detect Tools (tool_detection.rs)
    â”‚       â”œâ”€> Scan PATH
    â”‚       â””â”€> Parse history files
    â”‚
    â”œâ”€> Fetch Agentic Context (context_agent.rs)
    â”‚       â”œâ”€> Analyze query keywords
    â”‚       â”œâ”€> Fetch relevant context:
    â”‚       â”‚   â”œâ”€> Git (if query mentions git)
    â”‚       â”‚   â”œâ”€> Docker (if query mentions docker)
    â”‚       â”‚   â”œâ”€> NPM (if query mentions npm/node)
    â”‚       â”‚   â”œâ”€> Python (if query mentions python)
    â”‚       â”‚   â”œâ”€> Shell RC (if query mentions alias/function)
    â”‚       â”‚   â”œâ”€> Cargo (if query mentions rust/cargo)
    â”‚       â”‚   â””â”€> K8s (if query mentions kubectl/k8s)
    â”‚       â””â”€> Combine contexts
    â”‚
    â”œâ”€> Build Full Prompt
    â”‚       â”œâ”€> System prompt (safety rules, format)
    â”‚       â”œâ”€> System context
    â”‚       â”œâ”€> Agentic context
    â”‚       â””â”€> User query
    â”‚
    â”œâ”€> Call LLM (llm_client.rs)
    â”‚       â”œâ”€> Ollama (local)
    â”‚       â”œâ”€> Groq (cloud)
    â”‚       â””â”€> OpenAI (cloud)
    â”‚
    â”œâ”€> Parse Response
    â”‚       â”œâ”€> Try parse as JSON
    â”‚       â”œâ”€> Extract command & description
    â”‚       â””â”€> Fallback to plain text
    â”‚
    â””â”€> Output
            â”œâ”€> Print description (ðŸ’¡)
            â””â”€> Print command
```

### 2. Configuration Flow

```
Startup
    â”‚
    â”œâ”€> Check ~/.config/ez-term/config.toml exists
    â”‚       â”œâ”€> Yes: Load TOML
    â”‚       â””â”€> No: Use defaults
    â”‚
    â”œâ”€> Check Environment Variables
    â”‚       â”œâ”€> GROQ_API_KEY
    â”‚       â”œâ”€> OPENAI_API_KEY
    â”‚       â””â”€> OLLAMA_HOST
    â”‚
    â””â”€> Merge (env vars override config file)
            â”œâ”€> Backend selection
            â”œâ”€> Model selection
            â””â”€> API keys
```

## Key Components

### 1. CLI Parser (`cli.rs`)

**Purpose**: Parse command-line arguments using clap

**Key Fields**:
- `query: Option<String>` - User's natural language query
- `backend: Option<String>` - Override backend (ollama/groq/openai)
- `model: Option<String>` - Override model
- `list_backends: bool` - List available backends
- `set_backend: Option<String>` - Set default backend
- `update: bool` - Trigger self-update

### 2. Configuration (`config.rs`)

**Purpose**: Manage persistent configuration

**Storage**: `~/.config/ez-term/config.toml`

**Fields**:
```rust
pub struct Config {
    pub backend: Option<String>,
    pub model: Option<String>,
    pub ollama_url: Option<String>,
    pub groq_api_key: Option<String>,
    pub openai_api_key: Option<String>,
}
```

**Priority**:
1. Environment variables (highest)
2. Config file
3. Defaults (lowest)

### 3. System Info (`system_info.rs`)

**Purpose**: Detect OS and system characteristics

**Collected Data**:
- OS name (linux/macos/windows)
- Architecture (x86_64/aarch64)
- Shell (bash/zsh/fish)
- Memory (total/available)
- OS-specific command guidance

### 4. Tool Detection (`tool_detection.rs`)

**Purpose**: Detect installed tools and user preferences

**Scans**:
- PATH directories for common tools
- Shell history files (.bash_history, .zsh_history)
- Frequency analysis of commands

**Output**:
- List of installed tools
- Top 20 most frequent commands

### 5. Context Agent (`context_agent.rs`)

**Purpose**: Intelligently fetch relevant context based on query

**Detectors**:
- Git: branch, user, remotes, uncommitted files
- Docker: version, running containers, compose files
- NPM: node version, package.json, scripts
- Python: version, venv, requirements files
- Shell: aliases, functions, exports from RC files
- Cargo: rust version, Cargo.toml
- Kubernetes: context, namespace

**Trigger**: Keyword pattern matching in user query

### 6. LLM Client (`llm_client.rs`)

**Purpose**: Abstract interface to multiple LLM backends

**Supported Backends**:
- **Ollama**: Local models via HTTP API
- **Groq**: Cloud API (fast inference)
- **OpenAI**: Cloud API (GPT models)

**Methods**:
- `generate_and_collect()` - Send prompt, collect full response
- `list_models()` - Get available models for backend

**Response Format**:
```json
{
  "command": "ls -la",
  "description": "Lists all files with details"
}
```

### 7. Update System (`update.rs`)

**Purpose**: Self-update from GitHub releases

**Flow**:
1. Detect current version (from Cargo.toml)
2. Query GitHub API for latest release
3. Detect platform (linux/macos/windows)
4. Download appropriate binary
5. Replace running binary
6. Verify installation

## Design Decisions

### Why Rust?

- **Performance**: Fast startup, minimal overhead
- **Safety**: Memory safe, no runtime errors
- **Cross-platform**: Single binary for Linux/macOS/Windows
- **Ecosystem**: Excellent CLI libraries (clap, tokio, reqwest)

### Why Multiple Backends?

- **Flexibility**: Users choose based on needs
- **Offline**: Ollama works without internet
- **Quality**: OpenAI for best results
- **Speed**: Groq for fast responses
- **Cost**: Free tier options (Groq, Ollama)

### Why Agentic Context?

- **Relevance**: Only fetch context that matters
- **Performance**: Don't parse everything upfront
- **Accuracy**: LLM gets targeted, specific information
- **Privacy**: Only read what's needed

### Why Local Context Fetching?

- **Privacy**: No data sent except LLM prompt
- **Speed**: Local filesystem reads are fast
- **Offline**: Works without internet (with Ollama)
- **Control**: User owns all data

## Extension Points

### Adding a New Backend

1. Add enum variant to `Backend` in `llm_client.rs`:
```rust
pub enum Backend {
    Ollama,
    Groq,
    OpenAI,
    NewBackend,  // Add here
}
```

2. Implement backend methods:
```rust
async fn generate_and_collect_newbackend(
    &self,
    prompt: &str,
    system_context: &str
) -> Result<String> {
    // Implementation
}
```

3. Add to `list_models()` and `default_model()`

4. Update documentation in README.md

### Adding a New Context Detector

1. Add detection method in `context_agent.rs`:
```rust
fn get_tool_context(&self) -> Option<String> {
    // Check if tool is installed
    // Fetch relevant information
    // Return formatted context
}
```

2. Add keywords to trigger detection:
```rust
let needs_tool = self.mentions_tool(
    &query_lower,
    &["keyword1", "keyword2"]
);
```

3. Integrate in `get_relevant_context()`:
```rust
if needs_tool {
    if let Some(ctx) = self.get_tool_context() {
        context.push_str("\n## Tool Context\n");
        context.push_str(&ctx);
    }
}
```

4. Add tests in `tests/test_context_agent.rs`

5. Document in `docs/AGENTIC_CONTEXT.md`

### Adding System Information

1. Add field to `SystemInfo` struct in `system_info.rs`:
```rust
pub struct SystemInfo {
    pub os: String,
    pub new_field: String,  // Add here
}
```

2. Detect in `SystemInfo::detect()`:
```rust
let new_field = detect_new_field();
```

3. Format in `format_context()`:
```rust
context.push_str(&format!("\nNew Field: {}", self.new_field));
```

4. Add tests for new detection

## Testing Architecture

```
tests/
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ cli_tests.rs           # CLI argument parsing
â”‚   â”œâ”€â”€ config_tests.rs        # Config persistence
â”‚   â””â”€â”€ llm_client_tests.rs    # Backend integration
â”‚
â”œâ”€â”€ e2e_command_generation.rs  # Full command generation flow
â”œâ”€â”€ e2e_update.rs              # Update functionality
â””â”€â”€ test_context_agent.rs      # Context detection

src/
â”œâ”€â”€ config.rs                  # Unit tests inline
â”œâ”€â”€ system_info.rs             # Unit tests inline
â””â”€â”€ tool_detection.rs          # Unit tests inline
```

### Test Strategy

- **Unit Tests**: Test individual functions in isolation
- **Integration Tests**: Test module interactions
- **E2E Tests**: Test complete user workflows
- **Mock External Dependencies**: Use httpmock for API calls

## Performance Considerations

### Startup Time

- Config loading: ~1ms
- System detection: ~5ms
- Tool detection: ~10ms (PATH scan + history)
- Context agent: ~50ms (only when triggered)

**Total**: <100ms cold start

### Memory Usage

- Base binary: ~5MB
- Runtime memory: ~10-20MB
- Negligible for modern systems

### Optimization Strategies

1. **Lazy Loading**: Context agent only runs when needed
2. **Caching**: Tool detection results cached per session
3. **Async I/O**: Non-blocking LLM calls
4. **Minimal Dependencies**: Only essential crates

## Security Considerations

### API Keys

- Stored in config file with user-only permissions (600)
- Environment variables preferred for CI/CD
- Never logged or printed

### Command Execution

- Generated commands are displayed, NOT executed
- User must review before running
- Shell wrapper injects into readline buffer

### Context Privacy

- All context fetching is local
- No telemetry or analytics
- Only LLM prompt contains context

### Update Security

- Binaries downloaded from official GitHub releases
- HTTPS only
- Checksum verification (TODO)

## Future Architecture Enhancements

### Planned Improvements

1. **Plugin System**
   - Load external context detectors
   - Custom backend implementations

2. **Caching Layer**
   - Cache LLM responses for identical queries
   - Persistent context cache

3. **Streaming Support**
   - Real-time command generation
   - Progressive output

4. **Multi-step Commands**
   - Chain multiple commands
   - Conditional execution

5. **Learning System**
   - Learn from user's command selections
   - Personalized suggestions

## Contributing to Architecture

Want to extend the architecture? See:

- [CONTRIBUTING.md](../CONTRIBUTING.md) - Contribution guidelines
- [docs/TESTING.md](./TESTING.md) - Testing guide
- [docs/AGENTIC_CONTEXT.md](./AGENTIC_CONTEXT.md) - Context system details

## Questions?

Open an issue for architecture discussions or questions!
